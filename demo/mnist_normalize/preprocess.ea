// MNIST preprocessing kernels â€” normalize, foreach, and fused pipeline.
// All three exports in one compilation unit.

// foreach variant (relies on LLVM auto-vectorization at O2+)
export func normalize_foreach(input: *restrict f32, out: *mut f32, len: i32, scale: f32) {
    foreach (i in 0..len) {
        out[i] = input[i] * scale
    }
}

// Explicit SIMD normalization: uint8 [0,255] -> float32 [0,1] via caller-provided scale
export func normalize_f32x8(input: *restrict f32, out: *mut f32, len: i32, scale: f32) {
    let vscale: f32x8 = splat(scale)
    let mut i: i32 = 0
    while i + 8 <= len {
        let v: f32x8 = load(input, i)
        store(out, i, v .* vscale)
        i = i + 8
    }
    while i < len {
        out[i] = input[i] * scale
        i = i + 1
    }
}

// Fused: normalize + standardize + clip in ONE pass.
// Replaces 3-4 NumPy memory passes with a single read+write.
export func preprocess_fused(
    input: *restrict f32,
    out: *mut f32,
    len: i32,
    scale: f32,
    mean: f32,
    inv_std: f32
) {
    let vscale: f32x8 = splat(scale)
    let vmean: f32x8 = splat(mean)
    let vinv_std: f32x8 = splat(inv_std)
    let vzero: f32x8 = splat(0.0)
    let vone: f32x8 = splat(1.0)
    let mut i: i32 = 0
    while i + 8 <= len {
        let v: f32x8 = load(input, i)
        let norm: f32x8 = v .* vscale
        let centered: f32x8 = norm .- vmean
        let scaled: f32x8 = centered .* vinv_std
        let clamped_lo: f32x8 = select(scaled .< vzero, vzero, scaled)
        let clamped: f32x8 = select(clamped_lo .> vone, vone, clamped_lo)
        store(out, i, clamped)
        i = i + 8
    }
    while i < len {
        let norm: f32 = input[i] * scale
        let std_val: f32 = (norm - mean) * inv_std
        let mut result: f32 = std_val
        if result < 0.0 { result = 0.0 }
        if result > 1.0 { result = 1.0 }
        out[i] = result
        i = i + 1
    }
}
